{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Классификация изображений: распознавание персонажей Симпсонов.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\n\nimport PIL\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nimport numpy as np\nfrom skimage import io\nimport random\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = torch.device(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset construction"},{"metadata":{},"cell_type":"markdown","source":"Отнаследуемся от класса Dataset. Новый класс будет подгатавливать изображения — масштабировать, нормализовывать, аугментировать и т.д."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n  def __init__(self, files, mode, augmentations = None):\n    super().__init__()\n    self.files = files\n    self.mode = mode\n    self.augmentations = augmentations\n\n    if self.mode not in DATA_MODES:\n      print(f'wrong mode: {self.mode}')\n      raise NameError\n\n    self.len_ = len(self.files)\n    self.label_encoder = LabelEncoder()\n\n    if self.mode != 'test':\n      self.labels = [path.parent.name for path in self.files]\n      self.label_encoder.fit(self.labels)\n\n      with open('label_encoder.pkl', 'wb') as le_dump:\n        pickle.dump(self.label_encoder, le_dump)\n\n  def __len__(self):\n    return self.len_\n\n  def load_sample(self, file):\n    image = Image.open(file)\n    image.load()\n    return image\n\n  def __getitem__(self, index):\n    transform = transforms.Compose([\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                \n    ])\n\n    x = self.load_sample(self.files[index])\n    x = self._prepare_sample(x)\n    x = np.array(x / 255, dtype='float32')\n\n    x = transform(x)\n  \n    if self.mode == 'test':\n      return x\n    else:        \n      label = self.labels[index]\n      label_id = self.label_encoder.transform([label])\n      y = label_id.item()\n      return x, y\n\n  def _prepare_sample(self, image):\n    image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n    return np.array(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = Path('/kaggle/input/simpsons4/train')\nTEST_DIR = Path('/kaggle/input/simpsons4/testset/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.3, \\\n                                          stratify=train_val_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = SimpsonsDataset(val_files, mode='val')\ntrain_dataset = SimpsonsDataset(train_files, mode='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img, title=None, plt_ax=plt, default=False):\n  img = img.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  img = std * img + mean\n  img = np.clip(img, 0, 1)\n  plt_ax.imshow(img)\n  if title is not None:\n    plt_ax.set_title(title)\n  plt_ax.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10,10), sharex=True, sharey=True)\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's build the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvModel(nn.Module):\n  \n    def __init__(self, n_classes):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU()\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU()\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n            nn.BatchNorm2d(96),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU()\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=96, out_channels=164, kernel_size=3),\n            nn.BatchNorm2d(164),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU()\n        )\n        self.fc1 = nn.Sequential(\n            nn.Linear(5 * 5 * 164, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )  \n        self.fc4 = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU()\n        )  \n        self.out = nn.Linear(1024, n_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = x.view(-1, 5 * 5 * 164)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n\n        logits = self.out(x)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Функции для обучения и оценки модели."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss / processed_data\n    train_acc = running_corrects.cpu().numpy() / processed_data\n    return train_loss, train_acc\n  \ndef eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc\n  \ndef train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n      \n        opt = torch.optim.AdamW(model.parameters())\n\n        criterion = nn.CrossEntropyLoss()\n        \n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = len(np.unique(train_val_labels))\nmodel = ConvModel(n_classes)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = train(train_dataset, val_dataset, model=model, epochs=15, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"Предсказание, для одной картинки\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs\n\nrandom_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(model, ex_img.unsqueeze(0))\n\nidxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)\n\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ny_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\n\npreds_class = [label_encoder.classes_[i] for i in y_pred]\n\nfrom sklearn.metrics import f1_score\n\nf1_score(actual_labels, y_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ntest_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(model, test_loader)\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}